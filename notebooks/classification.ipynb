{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100]\n",
      "Training accuracy: 0.938\n",
      "Test accuracy: 0.950\n",
      "Loss: 0.1743\n",
      "\n",
      "Epoch [20/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.975\n",
      "Loss: 0.0740\n",
      "\n",
      "Epoch [30/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.975\n",
      "Loss: 0.0215\n",
      "\n",
      "Epoch [40/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.975\n",
      "Loss: 0.0228\n",
      "\n",
      "Epoch [50/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 1.000\n",
      "Loss: 0.0091\n",
      "\n",
      "Epoch [60/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.975\n",
      "Loss: 0.0078\n",
      "\n",
      "Epoch [70/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.975\n",
      "Loss: 0.0066\n",
      "\n",
      "Epoch [80/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 1.000\n",
      "Loss: 0.0033\n",
      "\n",
      "Epoch [90/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 0.975\n",
      "Loss: 0.0073\n",
      "\n",
      "Epoch [100/100]\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 1.000\n",
      "Loss: 0.0030\n",
      "\n",
      "\n",
      "Best test accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from chronos import ChronosPipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# If you want to use a neural network classifier instead\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "# 1. Load the Chronos model\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",  # You can use other model sizes too\n",
    "    device_map=\"cuda\",  # Use \"cpu\" for CPU inference\n",
    "    torch_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# 2. Function to extract embeddings for a dataset\n",
    "def extract_chronos_embeddings(time_series_list):\n",
    "    \"\"\"\n",
    "    Extract Chronos embeddings for a list of time series.\n",
    "    \n",
    "    Args:\n",
    "        time_series_list: List of time series (each as numpy array or torch tensor)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(time_series_list[0], np.ndarray):\n",
    "        time_series_list = [torch.tensor(x) for x in time_series_list]\n",
    "    \n",
    "    # Get embeddings using Chronos\n",
    "    embeddings, _ = pipeline.embed(time_series_list)\n",
    "    \n",
    "    # Average pooling over the time dimension to get a fixed-size representation\n",
    "    # You could also try max pooling or other pooling strategies\n",
    "    embeddings_pooled = embeddings.mean(dim=1)\n",
    "    \n",
    "    return embeddings_pooled.cpu().numpy()\n",
    "\n",
    "# 3. Example usage with a classification task\n",
    "def train_classifier(X_time_series, y_labels, num_epochs=100, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train a neural network classifier using Chronos embeddings\n",
    "    \"\"\"\n",
    "    # Extract embeddings\n",
    "    X_embeddings = extract_chronos_embeddings(X_time_series)\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_labels_encoded = label_encoder.fit_transform(y_labels)\n",
    "    # Convert to PyTorch tensors\n",
    "    X_embeddings = torch.FloatTensor(X_embeddings)\n",
    "    y_labels = torch.LongTensor(y_labels_encoded)  # Now using encoded labels\n",
    "    \n",
    "    # Split into train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_embeddings, y_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_classes = len(torch.unique(y_labels))\n",
    "    model = SimpleClassifier(\n",
    "        input_dim=X_embeddings.shape[1],\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=0.5\n",
    "    )\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_test_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Training accuracy\n",
    "            X_train_gpu = X_train.to(device)\n",
    "            train_outputs = model(X_train_gpu)\n",
    "            _, predicted = torch.max(train_outputs, 1)\n",
    "            train_acc = (predicted.cpu() == y_train).float().mean().item()\n",
    "            \n",
    "            # Test accuracy\n",
    "            X_test_gpu = X_test.to(device)\n",
    "            test_outputs = model(X_test_gpu)\n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            test_acc = (predicted.cpu() == y_test).float().mean().item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(train_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            best_state = model.state_dict()\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Training accuracy: {train_acc:.3f}')\n",
    "            print(f'Test accuracy: {test_acc:.3f}')\n",
    "            print(f'Loss: {train_loss/len(train_loader):.4f}\\n')\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f'\\nBest test accuracy: {best_test_acc:.3f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# 4. Example with UCR/UEA time series dataset\n",
    "def load_ucr_dataset(dataset_name):\n",
    "    \"\"\"\n",
    "    Load a dataset from the UCR/UEA archive\n",
    "    You'll need to download the dataset first\n",
    "    \"\"\"\n",
    "    from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "    \n",
    "    # Load train data\n",
    "    X_train, y_train = load_from_tsfile_to_dataframe(\n",
    "        dataset_name + \"_TRAIN\", return_separate_X_and_y=True\n",
    "    )\n",
    "    \n",
    "    # Load test data\n",
    "    X_test, y_test = load_from_tsfile_to_dataframe(\n",
    "        dataset_name + \"_TEST\", return_separate_X_and_y=True\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_train = np.array([x.values for x in X_train.iloc[:, 0]])\n",
    "    X_test = np.array([x.values for x in X_test.iloc[:, 0]])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Load a real dataset (GunPoint is a simple binary classification dataset)\n",
    "    dataset_name = \"GunPoint\"\n",
    "    X_train, y_train, X_test, y_test = load_ucr_dataset(dataset_name)\n",
    "    \n",
    "    # Combine train and test for our own train/test split\n",
    "    X_combined = np.concatenate([X_train, X_test])\n",
    "    y_combined = np.concatenate([y_train, y_test])\n",
    "    \n",
    "    # Train classifier\n",
    "    classifier = train_classifier(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2', '2', '1', '1', '2', '2', '2', '2', '2', '1', '1', '1', '1',\n",
       "       '1', '2', '1', '2', '2', '1', '2', '1', '1', '1', '2', '1', '2',\n",
       "       '1', '1', '2', '1', '1', '2', '2', '1', '2', '1', '2', '2', '2',\n",
       "       '2', '2', '1', '1', '1', '2', '2', '1', '2', '1', '2'], dtype='<U1')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_add_to_diagonal' from 'sklearn.utils._array_api' (/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/sklearn/utils/_array_api.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtslearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsTimeSeriesClassifier\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_dtw_classifier\u001b[39m(X_time_series, y_labels):\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/site-packages/tslearn/neighbors/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`tslearn.neighbors` module gathers nearest neighbor algorithms using\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mtime series metrics.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     KNeighborsTimeSeries,\n\u001b[1;32m      8\u001b[0m     KNeighborsTimeSeriesClassifier,\n\u001b[1;32m      9\u001b[0m     KNeighborsTimeSeriesRegressor\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNeighborsTimeSeries\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNeighborsTimeSeriesRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNeighborsTimeSeriesClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/site-packages/tslearn/neighbors/neighbors.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m neighbors\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (KNeighborsClassifier, NearestNeighbors,\n\u001b[1;32m      4\u001b[0m                                KNeighborsRegressor)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_array\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/site-packages/sklearn/__init__.py:133\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     87\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m ]\n\u001b[0;32m--> 133\u001b[0m _BUILT_WITH_MESON \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_built_with_meson\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/site-packages/sklearn/neighbors/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kde\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KernelDensity\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lof\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LocalOutlierFactor\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_nca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NeighborhoodComponentsAnalysis\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_nearest_centroid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestCentroid\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_regression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsRegressor, RadiusNeighborsRegressor\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/site-packages/sklearn/neighbors/_nca.py:23\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m minimize\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     BaseEstimator,\n\u001b[1;32m     19\u001b[0m     ClassNamePrefixFeaturesOutMixin,\n\u001b[1;32m     20\u001b[0m     TransformerMixin,\n\u001b[1;32m     21\u001b[0m     _fit_context,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvergenceWarning\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/site-packages/sklearn/decomposition/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_factor_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactorAnalysis\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fastica\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastICA, fastica\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_incremental_pca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IncrementalPCA\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kernel_pca\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KernelPCA\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LatentDirichletAllocation\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/site-packages/sklearn/decomposition/_incremental_pca.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _incremental_mean_and_var, svd_flip\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BasePCA\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIncrementalPCA\u001b[39;00m(_BasePCA):\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Incremental principal components analysis (IPCA).\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    Linear dimensionality reduction using Singular Value Decomposition of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    (1797, 7)\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/chronos/lib/python3.10/site-packages/sklearn/decomposition/_base.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, ClassNamePrefixFeaturesOutMixin, TransformerMixin\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_to_diagonal, device, get_namespace\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_is_fitted\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_BasePCA\u001b[39;00m(\n\u001b[1;32m     22\u001b[0m     ClassNamePrefixFeaturesOutMixin, TransformerMixin, BaseEstimator, metaclass\u001b[38;5;241m=\u001b[39mABCMeta\n\u001b[1;32m     23\u001b[0m ):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_add_to_diagonal' from 'sklearn.utils._array_api' (/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/sklearn/utils/_array_api.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "import time\n",
    "\n",
    "def train_dtw_classifier(X_time_series, y_labels):\n",
    "    \"\"\"\n",
    "    Train a DTW-based k-NN classifier\n",
    "    \n",
    "    Args:\n",
    "        X_time_series: List of time series data\n",
    "        y_labels: List/array of corresponding labels\n",
    "    \"\"\"\n",
    "    # Split into train/test (using same split as Chronos for fair comparison)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_time_series, y_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "    X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "    X_test_2d = scaler.transform(X_test_2d)\n",
    "    X_train = X_train_2d.reshape(X_train.shape)\n",
    "    X_test = X_test_2d.reshape(X_test.shape)\n",
    "    \n",
    "    # Train DTW-kNN classifier\n",
    "    start_time = time.time()\n",
    "    knn_dtw = KNeighborsTimeSeriesClassifier(n_neighbors=3, metric=\"dtw\")\n",
    "    knn_dtw.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    start_time = time.time()\n",
    "    train_score = knn_dtw.score(X_train, y_train)\n",
    "    test_score = knn_dtw.score(X_test, y_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\nDTW-kNN Results:\")\n",
    "    print(f\"Training accuracy: {train_score:.3f}\")\n",
    "    print(f\"Test accuracy: {test_score:.3f}\")\n",
    "    print(f\"Training time: {train_time:.2f} seconds\")\n",
    "    print(f\"Inference time: {inference_time:.2f} seconds\")\n",
    "    \n",
    "    return knn_dtw\n",
    "\n",
    "# Modified main section to compare both methods\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    dataset_name = \"GunPoint\"\n",
    "    X_train, y_train, X_test, y_test = load_ucr_dataset(dataset_name)\n",
    "    \n",
    "    # Combine train and test for our own train/test split\n",
    "    X_combined = np.concatenate([X_train, X_test])\n",
    "    y_combined = np.concatenate([y_train, y_test])\n",
    "    \n",
    "    print(\"Dataset shape:\", X_combined.shape)\n",
    "    print(\"Number of classes:\", len(np.unique(y_combined)))\n",
    "    \n",
    "    # Train Chronos-based classifier\n",
    "    print(\"\\nChronos-based Classification:\")\n",
    "    start_time = time.time()\n",
    "    chronos_classifier = train_classifier(X_combined, y_combined)\n",
    "    print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Train DTW-based classifier\n",
    "    print(\"\\nDTW-based Classification:\")\n",
    "    dtw_classifier = train_dtw_classifier(X_combined, y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
