{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-base\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Create sample input\n",
    "context = torch.randn(1, 100)  # Single time series of length 100\n",
    "prediction_length = 12  # Number of steps to predict\n",
    "\n",
    "# B: Get tokenized input\n",
    "# Reference: MeanScaleUniformBins.context_input_transform()\n",
    "token_ids, attention_mask, tokenizer_state = pipeline.tokenizer.context_input_transform(context)\n",
    "print(\"B - Tokenized shape:\", token_ids.shape)\n",
    "\n",
    "# C: Get encoder embeddings\n",
    "# Reference: ChronosModel.encode()\n",
    "encoder_output = pipeline.model.encode(\n",
    "    input_ids=token_ids.to(pipeline.model.device),\n",
    "    attention_mask=attention_mask.to(pipeline.model.device)\n",
    ")\n",
    "print(\"C - Encoder output shape:\", encoder_output.shape)\n",
    "\n",
    "# D: Get decoder output with prediction length\n",
    "# Create decoder input with start token\n",
    "decoder_input_ids = torch.full(\n",
    "    (token_ids.shape[0], prediction_length),  # [batch_size, prediction_length]\n",
    "    fill_value=pipeline.model.model.config.decoder_start_token_id,\n",
    "    device=encoder_output.device\n",
    ")\n",
    "\n",
    "decoder_output = pipeline.model.model.decoder(\n",
    "    input_ids=decoder_input_ids,\n",
    "    encoder_hidden_states=encoder_output,\n",
    "    encoder_attention_mask=attention_mask.to(encoder_output.device),\n",
    "    return_dict=True\n",
    ")\n",
    "print(\"D - Decoder output shape:\", decoder_output.last_hidden_state.shape)\n",
    "\n",
    "# E: Get output tokens using the language modeling head\n",
    "lm_logits = pipeline.model.model.lm_head(decoder_output.last_hidden_state)\n",
    "print(\"E - Output token logits shape:\", lm_logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0967,  0.0016,  0.0330,  ..., -0.0708, -0.0889, -0.0261],\n",
       "         [-0.0752,  0.0096,  0.0237,  ...,  0.0342,  0.0017, -0.0723],\n",
       "         [-0.1055,  0.0087, -0.0703,  ...,  0.0820,  0.0217,  0.0126],\n",
       "         ...,\n",
       "         [ 0.0708, -0.0554,  0.0576,  ..., -0.0376, -0.0272,  0.0284],\n",
       "         [ 0.1167,  0.0269,  0.0835,  ...,  0.0002, -0.0222,  0.0732],\n",
       "         [-0.0001, -0.0103,  0.0417,  ...,  0.1206, -0.0071, -0.0322]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0967,  0.0016,  0.0330,  ..., -0.0708, -0.0889, -0.0261],\n",
       "         [-0.0752,  0.0096,  0.0237,  ...,  0.0342,  0.0017, -0.0723],\n",
       "         [-0.1055,  0.0087, -0.0703,  ...,  0.0820,  0.0217,  0.0126],\n",
       "         ...,\n",
       "         [ 0.0708, -0.0554,  0.0576,  ..., -0.0376, -0.0272,  0.0284],\n",
       "         [ 0.1167,  0.0269,  0.0835,  ...,  0.0002, -0.0222,  0.0732],\n",
       "         [-0.0001, -0.0103,  0.0417,  ...,  0.1206, -0.0071, -0.0322]]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.embed(context)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 4096])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# Initialize teacher (larger) and student (smaller) models\n",
    "teacher = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",  # Larger model as teacher\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "student = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-tiny\",  # Smaller model as student\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "def get_model_outputs(pipeline, context, prediction_length):\n",
    "    # Get all intermediate representations\n",
    "    token_ids, attention_mask, tokenizer_state = pipeline.tokenizer.context_input_transform(context)\n",
    "    \n",
    "    encoder_output = pipeline.model.encode(\n",
    "        input_ids=token_ids.to(pipeline.model.device),\n",
    "        attention_mask=attention_mask.to(pipeline.model.device)\n",
    "    )\n",
    "    \n",
    "    decoder_input_ids = torch.full(\n",
    "        (token_ids.shape[0], prediction_length),\n",
    "        fill_value=pipeline.model.model.config.decoder_start_token_id,\n",
    "        device=encoder_output.device\n",
    "    )\n",
    "    \n",
    "    decoder_output = pipeline.model.model.decoder(\n",
    "        input_ids=decoder_input_ids,\n",
    "        encoder_hidden_states=encoder_output,\n",
    "        encoder_attention_mask=attention_mask.to(encoder_output.device),\n",
    "        return_dict=True\n",
    "    )\n",
    "    \n",
    "    lm_logits = pipeline.model.model.lm_head(decoder_output.last_hidden_state)\n",
    "    output_tokens = torch.argmax(lm_logits, dim=-1)\n",
    "    \n",
    "    return {\n",
    "        'token_ids': token_ids,\n",
    "        'encoder_output': encoder_output,\n",
    "        'decoder_output': decoder_output.last_hidden_state,\n",
    "        'logits': lm_logits,\n",
    "        'tokens': output_tokens\n",
    "    }\n",
    "\n",
    "def distillation_loss(teacher_outputs, student_outputs, temperature=2.0):\n",
    "    losses = {}\n",
    "    \n",
    "    # 1. Soft targets loss on logits\n",
    "    soft_targets = F.softmax(teacher_outputs['logits'] / temperature, dim=-1)\n",
    "    student_logits = student_outputs['logits'] / temperature\n",
    "    losses['soft_targets'] = F.kl_div(\n",
    "        F.log_softmax(student_logits, dim=-1),\n",
    "        soft_targets,\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "    \n",
    "    # 2. Hidden state distillation (encoder)\n",
    "    losses['encoder'] = F.mse_loss(\n",
    "        student_outputs['encoder_output'],\n",
    "        teacher_outputs['encoder_output']\n",
    "    )\n",
    "    \n",
    "    # 3. Hidden state distillation (decoder)\n",
    "    losses['decoder'] = F.mse_loss(\n",
    "        student_outputs['decoder_output'],\n",
    "        teacher_outputs['decoder_output']\n",
    "    )\n",
    "    \n",
    "    # 4. Hard prediction loss\n",
    "    losses['tokens'] = F.cross_entropy(\n",
    "        student_outputs['logits'].view(-1, student_outputs['logits'].size(-1)),\n",
    "        teacher_outputs['tokens'].view(-1)\n",
    "    )\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Training loop\n",
    "def train_step(context, prediction_length):\n",
    "    # Get teacher outputs (with no grad)\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = get_model_outputs(teacher, context, prediction_length)\n",
    "    \n",
    "    # Get student outputs\n",
    "    student_outputs = get_model_outputs(student, context, prediction_length)\n",
    "    \n",
    "    # Calculate losses\n",
    "    losses = distillation_loss(teacher_outputs, student_outputs)\n",
    "    \n",
    "    # Total loss (with weights for each component)\n",
    "    total_loss = (\n",
    "        0.5 * losses['soft_targets'] +\n",
    "        0.1 * losses['encoder'] +\n",
    "        0.1 * losses['decoder'] +\n",
    "        0.3 * losses['tokens']\n",
    "    )\n",
    "    \n",
    "    return total_loss, losses\n",
    "\n",
    "# Example usage\n",
    "context = torch.randn(1, 100)  # Sample input\n",
    "prediction_length = 12\n",
    "\n",
    "loss, component_losses = train_step(context, prediction_length)\n",
    "print(\"Total Loss:\", loss.item())\n",
    "print(\"Component Losses:\", {k: v.item() for k, v in component_losses.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
