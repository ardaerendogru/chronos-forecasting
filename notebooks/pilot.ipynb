{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from gluonts.dataset.arrow import ArrowWriter\n",
    "\n",
    "\n",
    "def convert_to_arrow_chunked(\n",
    "    path: Union[str, Path],\n",
    "    dataset,\n",
    "    chunk_size: int = 10000,\n",
    "    compression: str = \"lz4\",\n",
    "):\n",
    "    import pyarrow as pa\n",
    "    import pandas as pd\n",
    "    \n",
    "    first_chunk = True\n",
    "    \n",
    "    with pa.OSFile(path, 'wb') as f:\n",
    "        for i in range(0, len(dataset), chunk_size):\n",
    "            chunk = dataset[i:i + chunk_size]\n",
    "            chunk_data = []\n",
    "            \n",
    "            # Get the values as lists\n",
    "            ids = chunk['id']\n",
    "            targets = chunk['target']\n",
    "            timestamps = chunk['timestamp']\n",
    "            # Zip the values together and process each row\n",
    "            for id_val, target_val, timestamp_val in zip(ids, targets, timestamps):\n",
    "                chunk_data.append({\n",
    "                    \"start\": timestamp_val[0],\n",
    "                    \"target\": target_val\n",
    "                })\n",
    "            \n",
    "            # Convert to Arrow table\n",
    "            df_chunk = pd.DataFrame(chunk_data)\n",
    "            table_chunk = pa.Table.from_pandas(df_chunk)\n",
    "            \n",
    "            # Initialize writer with schema from first chunk\n",
    "            if first_chunk:\n",
    "                schema = table_chunk.schema\n",
    "                writer = pa.ipc.new_file(f, schema)\n",
    "                first_chunk = False\n",
    "            \n",
    "            # Write chunk\n",
    "            writer.write(table_chunk)\n",
    "            print(f\"Processed {i + len(chunk)} / {len(dataset)} rows\")\n",
    "            break\n",
    "        writer.close()\n",
    "\n",
    "# # Load and process dataset\n",
    "# ds = datasets.load_dataset(\"autogluon/chronos_datasets\", \"training_corpus_tsmixup_10m\", split=\"train\")\n",
    "# ds.set_format(\"numpy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3 / 10000000 rows\n",
      "Processed 10003 / 10000000 rows\n",
      "Processed 20003 / 10000000 rows\n",
      "Processed 30003 / 10000000 rows\n",
      "Processed 40003 / 10000000 rows\n",
      "Processed 50003 / 10000000 rows\n",
      "Processed 60003 / 10000000 rows\n",
      "Processed 70003 / 10000000 rows\n",
      "Processed 80003 / 10000000 rows\n",
      "Processed 90003 / 10000000 rows\n",
      "Processed 100003 / 10000000 rows\n",
      "Processed 110003 / 10000000 rows\n",
      "Processed 120003 / 10000000 rows\n",
      "Processed 130003 / 10000000 rows\n",
      "Processed 140003 / 10000000 rows\n",
      "Processed 150003 / 10000000 rows\n",
      "Processed 160003 / 10000000 rows\n",
      "Processed 170003 / 10000000 rows\n",
      "Processed 180003 / 10000000 rows\n",
      "Processed 190003 / 10000000 rows\n",
      "Processed 200003 / 10000000 rows\n",
      "Processed 210003 / 10000000 rows\n",
      "Processed 220003 / 10000000 rows\n",
      "Processed 230003 / 10000000 rows\n",
      "Processed 240003 / 10000000 rows\n",
      "Processed 250003 / 10000000 rows\n",
      "Processed 260003 / 10000000 rows\n",
      "Processed 270003 / 10000000 rows\n",
      "Processed 280003 / 10000000 rows\n",
      "Processed 290003 / 10000000 rows\n",
      "Processed 300003 / 10000000 rows\n",
      "Processed 310003 / 10000000 rows\n",
      "Processed 320003 / 10000000 rows\n",
      "Processed 330003 / 10000000 rows\n",
      "Processed 340003 / 10000000 rows\n",
      "Processed 350003 / 10000000 rows\n",
      "Processed 360003 / 10000000 rows\n",
      "Processed 370003 / 10000000 rows\n",
      "Processed 380003 / 10000000 rows\n",
      "Processed 390003 / 10000000 rows\n",
      "Processed 400003 / 10000000 rows\n",
      "Processed 410003 / 10000000 rows\n",
      "Processed 420003 / 10000000 rows\n",
      "Processed 430003 / 10000000 rows\n",
      "Processed 440003 / 10000000 rows\n",
      "Processed 450003 / 10000000 rows\n",
      "Processed 460003 / 10000000 rows\n",
      "Processed 470003 / 10000000 rows\n",
      "Processed 480003 / 10000000 rows\n",
      "Processed 490003 / 10000000 rows\n",
      "Processed 500003 / 10000000 rows\n",
      "Processed 510003 / 10000000 rows\n",
      "Processed 520003 / 10000000 rows\n",
      "Processed 530003 / 10000000 rows\n",
      "Processed 540003 / 10000000 rows\n",
      "Processed 550003 / 10000000 rows\n",
      "Processed 560003 / 10000000 rows\n",
      "Processed 570003 / 10000000 rows\n",
      "Processed 580003 / 10000000 rows\n",
      "Processed 590003 / 10000000 rows\n",
      "Processed 600003 / 10000000 rows\n",
      "Processed 610003 / 10000000 rows\n",
      "Processed 620003 / 10000000 rows\n",
      "Processed 630003 / 10000000 rows\n",
      "Processed 640003 / 10000000 rows\n",
      "Processed 650003 / 10000000 rows\n",
      "Processed 660003 / 10000000 rows\n",
      "Processed 670003 / 10000000 rows\n",
      "Processed 680003 / 10000000 rows\n",
      "Processed 690003 / 10000000 rows\n",
      "Processed 700003 / 10000000 rows\n",
      "Processed 710003 / 10000000 rows\n",
      "Processed 720003 / 10000000 rows\n",
      "Processed 730003 / 10000000 rows\n",
      "Processed 740003 / 10000000 rows\n",
      "Processed 750003 / 10000000 rows\n",
      "Processed 760003 / 10000000 rows\n",
      "Processed 770003 / 10000000 rows\n",
      "Processed 780003 / 10000000 rows\n",
      "Processed 790003 / 10000000 rows\n",
      "Processed 800003 / 10000000 rows\n",
      "Processed 810003 / 10000000 rows\n",
      "Processed 820003 / 10000000 rows\n",
      "Processed 830003 / 10000000 rows\n",
      "Processed 840003 / 10000000 rows\n",
      "Processed 850003 / 10000000 rows\n",
      "Processed 860003 / 10000000 rows\n",
      "Processed 870003 / 10000000 rows\n",
      "Processed 880003 / 10000000 rows\n",
      "Processed 890003 / 10000000 rows\n",
      "Processed 900003 / 10000000 rows\n",
      "Processed 910003 / 10000000 rows\n",
      "Processed 920003 / 10000000 rows\n",
      "Processed 930003 / 10000000 rows\n",
      "Processed 940003 / 10000000 rows\n",
      "Processed 950003 / 10000000 rows\n",
      "Processed 960003 / 10000000 rows\n",
      "Processed 970003 / 10000000 rows\n",
      "Processed 980003 / 10000000 rows\n",
      "Processed 990003 / 10000000 rows\n",
      "Processed 1000003 / 10000000 rows\n",
      "Processed 1010003 / 10000000 rows\n",
      "Processed 1020003 / 10000000 rows\n",
      "Processed 1030003 / 10000000 rows\n",
      "Processed 1040003 / 10000000 rows\n",
      "Processed 1050003 / 10000000 rows\n"
     ]
    }
   ],
   "source": [
    "# Convert to arrow format in chunks\n",
    "convert_to_arrow_chunked(\"./tsmixup-data.arrow\", ds, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5000 rows\n",
      "Processed 10000 rows\n",
      "Processed 15000 rows\n",
      "Processed 20000 rows\n",
      "Processed 25000 rows\n",
      "Processed 30000 rows\n",
      "Processed 35000 rows\n",
      "Processed 40000 rows\n",
      "Processed 45000 rows\n",
      "Processed 50000 rows\n",
      "Processed 55000 rows\n",
      "Processed 60000 rows\n",
      "Processed 65000 rows\n",
      "Processed 70000 rows\n",
      "Processed 75000 rows\n",
      "Processed 80000 rows\n",
      "Processed 85000 rows\n",
      "Processed 90000 rows\n",
      "Processed 95000 rows\n",
      "Processed 100000 rows\n",
      "Processed 105000 rows\n",
      "Processed 110000 rows\n",
      "Processed 115000 rows\n",
      "Processed 120000 rows\n",
      "Processed 125000 rows\n",
      "Processed 130000 rows\n",
      "Processed 135000 rows\n",
      "Processed 140000 rows\n",
      "Processed 145000 rows\n",
      "Processed 150000 rows\n",
      "Processed 155000 rows\n",
      "Processed 160000 rows\n",
      "Processed 165000 rows\n",
      "Processed 170000 rows\n",
      "Processed 175000 rows\n",
      "Processed 180000 rows\n",
      "Processed 185000 rows\n",
      "Processed 190000 rows\n",
      "Processed 195000 rows\n",
      "Processed 200000 rows\n",
      "Processed 205000 rows\n",
      "Processed 210000 rows\n",
      "Processed 215000 rows\n",
      "Processed 220000 rows\n",
      "Processed 225000 rows\n",
      "Processed 230000 rows\n",
      "Processed 235000 rows\n",
      "Processed 240000 rows\n",
      "Processed 245000 rows\n",
      "Processed 250000 rows\n",
      "Processed 255000 rows\n",
      "Processed 260000 rows\n",
      "Processed 265000 rows\n",
      "Processed 270000 rows\n",
      "Processed 275000 rows\n",
      "Processed 280000 rows\n",
      "Processed 285000 rows\n",
      "Processed 290000 rows\n",
      "Processed 295000 rows\n",
      "Processed 300000 rows\n",
      "Processed 305000 rows\n",
      "Processed 310000 rows\n",
      "Processed 315000 rows\n",
      "Processed 320000 rows\n",
      "Processed 325000 rows\n",
      "Processed 330000 rows\n",
      "Processed 335000 rows\n",
      "Processed 340000 rows\n",
      "Processed 345000 rows\n",
      "Processed 350000 rows\n",
      "Processed 355000 rows\n",
      "Processed 360000 rows\n",
      "Processed 365000 rows\n",
      "Processed 370000 rows\n",
      "Processed 375000 rows\n",
      "Processed 380000 rows\n",
      "Processed 385000 rows\n",
      "Processed 390000 rows\n",
      "Processed 395000 rows\n",
      "Processed 400000 rows\n",
      "Processed 405000 rows\n",
      "Processed 410000 rows\n",
      "Processed 415000 rows\n",
      "Processed 420000 rows\n"
     ]
    }
   ],
   "source": [
    "# Process in chunks\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "chunk_size = 5000  # Adjust this based on your available RAM\n",
    "schema = None\n",
    "first_chunk = True\n",
    "processed_rows = 0\n",
    "\n",
    "with pa.OSFile('tsmixup-data.arrow', 'wb') as f:\n",
    "    # Iterate over the streaming dataset directly\n",
    "    for chunk in ds.iter(batch_size=chunk_size):\n",
    "        # Convert to pandas and process\n",
    "        df_chunk = pd.DataFrame(chunk)\n",
    "        df_chunk['start'] = df_chunk['timestamp'].apply(lambda x: pd.to_datetime(x[0]))\n",
    "        df_chunk = df_chunk[['start', 'target']]  # Keep only needed columns\n",
    "        \n",
    "        # Convert to Arrow table\n",
    "        table_chunk = pa.Table.from_pandas(df_chunk)\n",
    "        \n",
    "        # Initialize writer with schema from first chunk\n",
    "        if first_chunk:\n",
    "            schema = table_chunk.schema\n",
    "            writer = pa.ipc.new_file(f, schema)\n",
    "            first_chunk = False\n",
    "    \n",
    "        # Write chunk\n",
    "        writer.write(table_chunk)\n",
    "        \n",
    "        # Update and print progress\n",
    "        processed_rows += len(df_chunk)\n",
    "        print(f\"Processed {processed_rows} rows\")\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-31 23:50:15,273 - /home/arda/Documents/chronos-forecasting/notebooks/../scripts/training/train.py - INFO - Using SEED: 4034910953\n",
      "2024-12-31 23:50:15,277 - /home/arda/Documents/chronos-forecasting/notebooks/../scripts/training/train.py - INFO - Logging dir: output/run-8\n",
      "2024-12-31 23:50:15,277 - /home/arda/Documents/chronos-forecasting/notebooks/../scripts/training/train.py - INFO - Loading and filtering 2 datasets for training: ['/home/arda/Documents/chronos-forecasting/data/kernelsynth-data.arrow', '/home/arda/Documents/chronos-forecasting/notebooks/tsmixup-data.arrow']\n",
      "2024-12-31 23:50:15,277 - /home/arda/Documents/chronos-forecasting/notebooks/../scripts/training/train.py - INFO - Mixing probabilities: [0.9, 0.1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arda/Documents/chronos-forecasting/notebooks/../scripts/training/train.py\", line 702, in <module>\n",
      "    app()\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/typer/main.py\", line 340, in __call__\n",
      "    raise e\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/typer/main.py\", line 323, in __call__\n",
      "    return get_command(self)(*args, **kwargs)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/click/core.py\", line 1161, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/typer/core.py\", line 680, in main\n",
      "    return _main(\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/typer/core.py\", line 198, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/click/core.py\", line 1443, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/click/core.py\", line 788, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/typer/main.py\", line 698, in wrapper\n",
      "    return callback(**use_params)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/typer_config/decorators.py\", line 96, in wrapped\n",
      "    return cmd(*args, **kwargs)\n",
      "  File \"/home/arda/Documents/chronos-forecasting/notebooks/../scripts/training/train.py\", line 602, in main\n",
      "    train_datasets = [\n",
      "  File \"/home/arda/Documents/chronos-forecasting/notebooks/../scripts/training/train.py\", line 609, in <listcomp>\n",
      "    FileDataset(path=Path(data_path), freq=\"h\"),\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/gluonts/dataset/common.py\", line 177, in FileDataset\n",
      "    loader = infer_file_type(subpath)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/gluonts/dataset/common.py\", line 125, in infer_file_type\n",
      "    return arrow.File.infer(path)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/gluonts/dataset/arrow/file.py\", line 47, in infer\n",
      "    return ArrowFile(path)\n",
      "  File \"<string>\", line 6, in __init__\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/gluonts/dataset/arrow/file.py\", line 93, in __post_init__\n",
      "    self.reader = pa.RecordBatchFileReader(self.path)\n",
      "  File \"/home/arda/anaconda3/envs/chronos/lib/python3.10/site-packages/pyarrow/ipc.py\", line 110, in __init__\n",
      "    self._open(source, footer_offset=footer_offset,\n",
      "  File \"pyarrow/ipc.pxi\", line 1090, in pyarrow.lib._RecordBatchFileReader._open\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Not an Arrow file\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/training/train.py --config ./config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (11000, 2)\n",
      "\n",
      "First few rows:\n",
      "       start                                             target\n",
      "0 1970-01-01  [0.613463059343371, 0.5711616398408842, 0.5182...\n",
      "1 1970-01-01  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...\n",
      "2 1970-01-01  [1.0770299311167582, 4.308119724467033, 1.0770...\n",
      "3 1970-01-01  [0.004624637833987918, 0.00547248810355237, 0....\n",
      "4 1970-01-01  [0.4764301971138915, 0.49643825495907135, 0.53...\n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11000 entries, 0 to 10999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   start   11000 non-null  datetime64[ns]\n",
      " 1   target  11000 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(1)\n",
      "memory usage: 172.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Open the Arrow file\n",
    "with pa.memory_map('tsmixup-data.arrow', 'r') as source:\n",
    "    reader = pa.ipc.open_file(source)\n",
    "    table = reader.read_all()\n",
    "\n",
    "# Convert to pandas DataFrame for easier inspection\n",
    "df = table.to_pandas()\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (100000, 2)\n",
      "\n",
      "First few rows:\n",
      "       start                                             target\n",
      "0 2000-01-01  [0.0847358005868161, 0.08490204073316547, 0.08...\n",
      "1 2000-01-01  [0.5819332221162387, 0.4920879847852638, 0.327...\n",
      "2 2000-01-01  [0.7310408977923173, 0.42255510512682065, 0.59...\n",
      "3 2000-01-01  [0.25454405428260085, 0.26002409188736336, 0.2...\n",
      "4 2000-01-01  [0.09628723019827937, 0.6191403183181314, -0.1...\n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype        \n",
      "---  ------  --------------   -----        \n",
      " 0   start   100000 non-null  datetime64[s]\n",
      " 1   target  100000 non-null  object       \n",
      "dtypes: datetime64[s](1), object(1)\n",
      "memory usage: 1.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "# Open the Arrow file\n",
    "with pa.memory_map('/home/arda/Documents/chronos-forecasting/data/kernelsynth-data.arrow', 'r') as source:\n",
    "    reader = pa.ipc.open_file(source)\n",
    "    table = reader.read_all()\n",
    "\n",
    "# Convert to pandas DataFrame for easier inspection\n",
    "df = table.to_pandas()\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nData info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
