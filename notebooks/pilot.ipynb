{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-06 13:05:35,094 - /home/arda/Documents/chronos-forecasting/scripts/training/train.py - INFO - Using SEED: 29438183\n",
      "2025-01-06 13:05:35,095 - /home/arda/Documents/chronos-forecasting/scripts/training/train.py - INFO - Logging dir: output/run-7\n",
      "2025-01-06 13:05:35,095 - /home/arda/Documents/chronos-forecasting/scripts/training/train.py - INFO - Loading and filtering 2 datasets for training: ['/home/arda/Documents/chronos-forecasting/data/tsmixup-data-10percent.arrow', '/home/arda/Documents/chronos-forecasting/data/kernelsynth-data-10percent.arrow']\n",
      "2025-01-06 13:05:35,095 - /home/arda/Documents/chronos-forecasting/scripts/training/train.py - INFO - Mixing probabilities: [0.9, 0.1]\n",
      "2025-01-06 13:05:35,097 - /home/arda/Documents/chronos-forecasting/scripts/training/train.py - INFO - Initializing model\n",
      "2025-01-06 13:05:35,097 - /home/arda/Documents/chronos-forecasting/scripts/training/train.py - INFO - Using random initialization\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python /home/arda/Documents/chronos-forecasting/scripts/training/train.py --config /home/arda/Documents/chronos-forecasting/scripts/training/configs/chronos-t5-tiny.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-07 08:02:01,231 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Using SEED: 2070782164\n",
      "2025-01-07 08:02:01,233 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Logging dir: output/run-17\n",
      "2025-01-07 08:02:01,233 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Loading and filtering 2 datasets for training: ['../data/tsmixup-data-10percent.arrow', '../data/kernelsynth-data-10percent.arrow']\n",
      "2025-01-07 08:02:01,233 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Mixing probabilities: [0.9, 0.1]\n",
      "2025-01-07 08:02:01,234 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Initializing model\n",
      "2025-01-07 08:02:01,234 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Using random initialization\n",
      "2025-01-07 08:02:11,784 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Loading teacher model\n",
      "2025-01-07 08:02:11,784 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Using pretrained initialization from amazon/chronos-t5-small\n",
      "2025-01-07 08:02:12,439 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Model dtype: torch.float32\n",
      "2025-01-07 08:02:12,699 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Training\n",
      "  0%|                                                | 0/100000 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "{'loss': 7.5123, 'grad_norm': 0.17125073075294495, 'learning_rate': 0.0009975000000000001, 'epoch': 0.0}\n",
      "{'loss': 6.0184, 'grad_norm': 0.09601376950740814, 'learning_rate': 0.000995, 'epoch': 0.01}\n",
      "{'loss': 5.5558, 'grad_norm': 0.14235852658748627, 'learning_rate': 0.0009925000000000001, 'epoch': 0.01}\n",
      "{'loss': 5.171, 'grad_norm': 0.14301882684230804, 'learning_rate': 0.00099, 'epoch': 0.01}\n",
      "{'loss': 4.9539, 'grad_norm': 0.2055380791425705, 'learning_rate': 0.0009875, 'epoch': 0.01}\n",
      "{'loss': 4.8283, 'grad_norm': 0.23068439960479736, 'learning_rate': 0.000985, 'epoch': 0.01}\n",
      "{'loss': 4.7485, 'grad_norm': 0.2939678728580475, 'learning_rate': 0.0009825, 'epoch': 0.02}\n",
      "{'loss': 4.672, 'grad_norm': 0.21550241112709045, 'learning_rate': 0.00098, 'epoch': 0.02}\n",
      "{'loss': 4.6005, 'grad_norm': 0.27065902948379517, 'learning_rate': 0.0009775, 'epoch': 0.02}\n",
      "{'loss': 4.5529, 'grad_norm': 0.2655111849308014, 'learning_rate': 0.000975, 'epoch': 0.03}\n",
      "{'loss': 4.5165, 'grad_norm': 0.36435195803642273, 'learning_rate': 0.0009725000000000001, 'epoch': 0.03}\n",
      "{'loss': 4.4869, 'grad_norm': 0.36884447932243347, 'learning_rate': 0.0009699999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.4344, 'grad_norm': 0.2741117477416992, 'learning_rate': 0.0009675, 'epoch': 0.03}\n",
      "{'loss': 4.4299, 'grad_norm': 0.36009636521339417, 'learning_rate': 0.000965, 'epoch': 0.04}\n",
      "{'loss': 4.3726, 'grad_norm': 0.3040185570716858, 'learning_rate': 0.0009625, 'epoch': 0.04}\n",
      "{'loss': 4.4085, 'grad_norm': 0.3240031898021698, 'learning_rate': 0.00096, 'epoch': 0.04}\n",
      "{'loss': 4.38, 'grad_norm': 0.2834124267101288, 'learning_rate': 0.0009575, 'epoch': 0.04}\n",
      "{'loss': 4.3751, 'grad_norm': 0.26067110896110535, 'learning_rate': 0.000955, 'epoch': 0.04}\n",
      "{'loss': 4.3466, 'grad_norm': 0.2413027286529541, 'learning_rate': 0.0009525, 'epoch': 0.05}\n",
      "{'loss': 4.3498, 'grad_norm': 0.3871750235557556, 'learning_rate': 0.00095, 'epoch': 0.05}\n",
      "{'loss': 4.3366, 'grad_norm': 0.30687233805656433, 'learning_rate': 0.0009475, 'epoch': 0.05}\n",
      "{'loss': 4.3249, 'grad_norm': 0.2631606161594391, 'learning_rate': 0.000945, 'epoch': 0.06}\n",
      "{'loss': 4.2922, 'grad_norm': 0.2905846834182739, 'learning_rate': 0.0009425, 'epoch': 0.06}\n",
      "{'loss': 4.2574, 'grad_norm': 0.31753993034362793, 'learning_rate': 0.00094, 'epoch': 0.06}\n",
      "{'loss': 4.2748, 'grad_norm': 0.30134105682373047, 'learning_rate': 0.0009375, 'epoch': 0.06}\n",
      "{'loss': 4.2735, 'grad_norm': 0.4149513840675354, 'learning_rate': 0.0009350000000000001, 'epoch': 0.07}\n",
      "{'loss': 4.2699, 'grad_norm': 0.30878809094429016, 'learning_rate': 0.0009325000000000001, 'epoch': 0.07}\n",
      "  7%|██▍                                | 6928/100000 [16:24<2:47:55,  9.24it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py --config /home/arda/Documents/chronos-forecasting/notebooks/distillation_copy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-07 10:35:42,761 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Using SEED: 1025626459\n",
      "2025-01-07 10:35:42,763 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Logging dir: output/run-23\n",
      "2025-01-07 10:35:42,764 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Loading and filtering 2 datasets for training: ['../data/tsmixup-data-10percent.arrow', '../data/kernelsynth-data-10percent.arrow']\n",
      "2025-01-07 10:35:42,764 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Mixing probabilities: [0.9, 0.1]\n",
      "2025-01-07 10:35:42,764 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Initializing model\n",
      "2025-01-07 10:35:42,764 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Using random initialization\n",
      "2025-01-07 10:35:53,065 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Loading teacher model\n",
      "2025-01-07 10:35:53,065 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Using pretrained initialization from amazon/chronos-t5-small\n",
      "2025-01-07 10:35:55,242 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Model dtype: torch.float32\n",
      "2025-01-07 10:35:55,351 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Training\n",
      "  0%|                                                | 0/100000 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "{'loss': 7.5115, 'grad_norm': 0.17279180884361267, 'learning_rate': 0.0009975000000000001, 'epoch': 0.0}\n",
      "{'loss': 6.0161, 'grad_norm': 0.09720300883054733, 'learning_rate': 0.000995, 'epoch': 0.01}\n",
      "{'loss': 5.55, 'grad_norm': 0.17935730516910553, 'learning_rate': 0.0009925000000000001, 'epoch': 0.01}\n",
      "{'loss': 5.1682, 'grad_norm': 0.16455836594104767, 'learning_rate': 0.00099, 'epoch': 0.01}\n",
      "{'loss': 4.9482, 'grad_norm': 0.20833754539489746, 'learning_rate': 0.0009875, 'epoch': 0.01}\n",
      "{'loss': 4.8265, 'grad_norm': 0.17837458848953247, 'learning_rate': 0.000985, 'epoch': 0.01}\n",
      "{'loss': 4.7498, 'grad_norm': 0.23537388443946838, 'learning_rate': 0.0009825, 'epoch': 0.02}\n",
      "{'loss': 4.6745, 'grad_norm': 0.21449537575244904, 'learning_rate': 0.00098, 'epoch': 0.02}\n",
      "{'loss': 4.6047, 'grad_norm': 0.2350539118051529, 'learning_rate': 0.0009775, 'epoch': 0.02}\n",
      "{'loss': 4.5536, 'grad_norm': 0.2690572142601013, 'learning_rate': 0.000975, 'epoch': 0.03}\n",
      "{'loss': 4.5218, 'grad_norm': 0.2768934965133667, 'learning_rate': 0.0009725000000000001, 'epoch': 0.03}\n",
      "{'loss': 4.4965, 'grad_norm': 0.3347732424736023, 'learning_rate': 0.0009699999999999999, 'epoch': 0.03}\n",
      "{'loss': 4.4455, 'grad_norm': 0.2392234206199646, 'learning_rate': 0.0009675, 'epoch': 0.03}\n",
      "{'loss': 4.4425, 'grad_norm': 0.31695473194122314, 'learning_rate': 0.000965, 'epoch': 0.04}\n",
      "{'loss': 4.3855, 'grad_norm': 0.295134961605072, 'learning_rate': 0.0009625, 'epoch': 0.04}\n",
      "{'loss': 4.4222, 'grad_norm': 0.31283900141716003, 'learning_rate': 0.00096, 'epoch': 0.04}\n",
      "{'loss': 4.3934, 'grad_norm': 0.2504792809486389, 'learning_rate': 0.0009575, 'epoch': 0.04}\n",
      "{'loss': 4.3914, 'grad_norm': 0.3273501396179199, 'learning_rate': 0.000955, 'epoch': 0.04}\n",
      "{'loss': 4.366, 'grad_norm': 0.25943249464035034, 'learning_rate': 0.0009525, 'epoch': 0.05}\n",
      "{'loss': 4.3708, 'grad_norm': 0.30925652384757996, 'learning_rate': 0.00095, 'epoch': 0.05}\n",
      "{'loss': 4.361, 'grad_norm': 0.3145780861377716, 'learning_rate': 0.0009475, 'epoch': 0.05}\n",
      "{'loss': 4.3499, 'grad_norm': 0.29315677285194397, 'learning_rate': 0.000945, 'epoch': 0.06}\n",
      "{'loss': 4.3208, 'grad_norm': 0.2939295768737793, 'learning_rate': 0.0009425, 'epoch': 0.06}\n",
      "{'loss': 4.2876, 'grad_norm': 0.3196874260902405, 'learning_rate': 0.00094, 'epoch': 0.06}\n",
      "{'loss': 4.3084, 'grad_norm': 0.2687298357486725, 'learning_rate': 0.0009375, 'epoch': 0.06}\n",
      "{'loss': 4.3097, 'grad_norm': 0.4258163273334503, 'learning_rate': 0.0009350000000000001, 'epoch': 0.07}\n",
      "{'loss': 4.3075, 'grad_norm': 0.2659379541873932, 'learning_rate': 0.0009325000000000001, 'epoch': 0.07}\n",
      "{'loss': 4.3198, 'grad_norm': 0.35423240065574646, 'learning_rate': 0.00093, 'epoch': 0.07}\n",
      "{'loss': 4.2873, 'grad_norm': 0.31551384925842285, 'learning_rate': 0.0009275, 'epoch': 0.07}\n",
      "{'loss': 4.278, 'grad_norm': 0.31847336888313293, 'learning_rate': 0.000925, 'epoch': 0.07}\n",
      "{'loss': 4.268, 'grad_norm': 0.32504647970199585, 'learning_rate': 0.0009225, 'epoch': 0.08}\n",
      "{'loss': 4.284, 'grad_norm': 0.35647284984588623, 'learning_rate': 0.00092, 'epoch': 0.08}\n",
      "{'loss': 4.2607, 'grad_norm': 0.4192909896373749, 'learning_rate': 0.0009175, 'epoch': 0.08}\n",
      "{'loss': 4.2569, 'grad_norm': 0.3518538773059845, 'learning_rate': 0.000915, 'epoch': 0.09}\n",
      "{'loss': 4.2682, 'grad_norm': 0.2683105170726776, 'learning_rate': 0.0009125, 'epoch': 0.09}\n",
      "{'loss': 4.241, 'grad_norm': 0.29518988728523254, 'learning_rate': 0.00091, 'epoch': 0.09}\n",
      "{'loss': 4.249, 'grad_norm': 0.3658364713191986, 'learning_rate': 0.0009075, 'epoch': 0.09}\n",
      "{'loss': 4.2017, 'grad_norm': 0.2830258905887604, 'learning_rate': 0.0009050000000000001, 'epoch': 0.1}\n",
      "{'loss': 4.2495, 'grad_norm': 0.33910465240478516, 'learning_rate': 0.0009025, 'epoch': 0.1}\n",
      "{'loss': 4.2432, 'grad_norm': 0.3377434015274048, 'learning_rate': 0.0009000000000000001, 'epoch': 0.1}\n",
      "{'loss': 4.2087, 'grad_norm': 0.31860190629959106, 'learning_rate': 0.0008975, 'epoch': 0.1}\n",
      "{'loss': 4.2324, 'grad_norm': 0.3987109661102295, 'learning_rate': 0.0008950000000000001, 'epoch': 0.1}\n",
      "{'loss': 4.2065, 'grad_norm': 0.31525978446006775, 'learning_rate': 0.0008925, 'epoch': 0.11}\n",
      "{'loss': 4.2056, 'grad_norm': 0.3092743456363678, 'learning_rate': 0.0008900000000000001, 'epoch': 0.11}\n",
      "{'loss': 4.2169, 'grad_norm': 0.38863110542297363, 'learning_rate': 0.0008874999999999999, 'epoch': 0.11}\n",
      "{'loss': 4.1942, 'grad_norm': 0.383602499961853, 'learning_rate': 0.000885, 'epoch': 0.12}\n",
      "{'loss': 4.2078, 'grad_norm': 0.3587830662727356, 'learning_rate': 0.0008824999999999999, 'epoch': 0.12}\n",
      "{'loss': 4.1662, 'grad_norm': 0.31159594655036926, 'learning_rate': 0.00088, 'epoch': 0.12}\n",
      "{'loss': 4.2163, 'grad_norm': 0.4877808392047882, 'learning_rate': 0.0008774999999999999, 'epoch': 0.12}\n",
      "{'loss': 4.2028, 'grad_norm': 0.2935104966163635, 'learning_rate': 0.000875, 'epoch': 0.12}\n",
      "{'loss': 4.1805, 'grad_norm': 0.41655051708221436, 'learning_rate': 0.0008725000000000001, 'epoch': 0.13}\n",
      "{'loss': 4.1556, 'grad_norm': 0.3601304292678833, 'learning_rate': 0.00087, 'epoch': 0.13}\n",
      "{'loss': 4.1738, 'grad_norm': 0.3872391879558563, 'learning_rate': 0.0008675000000000001, 'epoch': 0.13}\n",
      "{'loss': 4.1692, 'grad_norm': 0.29603105783462524, 'learning_rate': 0.000865, 'epoch': 0.14}\n",
      "{'loss': 4.1787, 'grad_norm': 0.35699358582496643, 'learning_rate': 0.0008625000000000001, 'epoch': 0.14}\n",
      "{'loss': 4.1632, 'grad_norm': 0.3476415276527405, 'learning_rate': 0.00086, 'epoch': 0.14}\n",
      "{'loss': 4.1636, 'grad_norm': 0.38691842555999756, 'learning_rate': 0.0008575000000000001, 'epoch': 0.14}\n",
      "{'loss': 4.154, 'grad_norm': 0.3167075514793396, 'learning_rate': 0.000855, 'epoch': 0.14}\n",
      "{'loss': 4.1533, 'grad_norm': 0.37938395142555237, 'learning_rate': 0.0008525000000000001, 'epoch': 0.15}\n",
      "{'loss': 4.1393, 'grad_norm': 0.3475300967693329, 'learning_rate': 0.00085, 'epoch': 0.15}\n",
      "{'loss': 4.1663, 'grad_norm': 0.3346467614173889, 'learning_rate': 0.0008475000000000001, 'epoch': 0.15}\n",
      "{'loss': 4.1579, 'grad_norm': 0.3221566081047058, 'learning_rate': 0.0008449999999999999, 'epoch': 0.15}\n",
      "{'loss': 4.1595, 'grad_norm': 0.4662185609340668, 'learning_rate': 0.0008425, 'epoch': 0.16}\n",
      "{'loss': 4.1407, 'grad_norm': 0.3772840201854706, 'learning_rate': 0.00084, 'epoch': 0.16}\n",
      "{'loss': 4.1399, 'grad_norm': 0.40536683797836304, 'learning_rate': 0.0008375, 'epoch': 0.16}\n",
      "{'loss': 4.1505, 'grad_norm': 0.41205930709838867, 'learning_rate': 0.000835, 'epoch': 0.17}\n",
      "{'loss': 4.1666, 'grad_norm': 0.3541737198829651, 'learning_rate': 0.0008325, 'epoch': 0.17}\n",
      "{'loss': 4.1249, 'grad_norm': 0.40523114800453186, 'learning_rate': 0.00083, 'epoch': 0.17}\n",
      "{'loss': 4.093, 'grad_norm': 0.37997859716415405, 'learning_rate': 0.0008275, 'epoch': 0.17}\n",
      "{'loss': 4.134, 'grad_norm': 0.4280519485473633, 'learning_rate': 0.000825, 'epoch': 0.17}\n",
      "{'loss': 4.1191, 'grad_norm': 0.3990964889526367, 'learning_rate': 0.0008225, 'epoch': 0.18}\n",
      "{'loss': 4.1204, 'grad_norm': 0.4342220425605774, 'learning_rate': 0.00082, 'epoch': 0.18}\n",
      "{'loss': 4.125, 'grad_norm': 0.336160272359848, 'learning_rate': 0.0008175, 'epoch': 0.18}\n",
      "{'loss': 4.0912, 'grad_norm': 0.42149823904037476, 'learning_rate': 0.000815, 'epoch': 0.18}\n",
      "{'loss': 4.1279, 'grad_norm': 0.42374101281166077, 'learning_rate': 0.0008125000000000001, 'epoch': 0.19}\n",
      "{'loss': 4.1203, 'grad_norm': 0.4785871207714081, 'learning_rate': 0.0008100000000000001, 'epoch': 0.19}\n",
      "{'loss': 4.1218, 'grad_norm': 0.3871460556983948, 'learning_rate': 0.0008075000000000001, 'epoch': 0.19}\n",
      "{'loss': 4.1219, 'grad_norm': 0.32752761244773865, 'learning_rate': 0.000805, 'epoch': 0.2}\n",
      "{'loss': 4.1294, 'grad_norm': 0.4563820958137512, 'learning_rate': 0.0008025, 'epoch': 0.2}\n",
      "{'loss': 4.1419, 'grad_norm': 0.43967509269714355, 'learning_rate': 0.0008, 'epoch': 0.2}\n",
      "{'loss': 4.108, 'grad_norm': 0.627778947353363, 'learning_rate': 0.0007975, 'epoch': 0.2}\n",
      "{'loss': 4.1045, 'grad_norm': 0.37035998702049255, 'learning_rate': 0.000795, 'epoch': 0.2}\n",
      "{'loss': 4.1093, 'grad_norm': 0.394473135471344, 'learning_rate': 0.0007925, 'epoch': 0.21}\n",
      "{'loss': 4.0995, 'grad_norm': 0.4104354977607727, 'learning_rate': 0.00079, 'epoch': 0.21}\n",
      "{'loss': 4.1117, 'grad_norm': 0.37168294191360474, 'learning_rate': 0.0007875, 'epoch': 0.21}\n",
      "{'loss': 4.0948, 'grad_norm': 0.2820020914077759, 'learning_rate': 0.000785, 'epoch': 0.21}\n",
      "{'loss': 4.0882, 'grad_norm': 0.3772069811820984, 'learning_rate': 0.0007825, 'epoch': 0.22}\n",
      "{'loss': 4.0763, 'grad_norm': 0.38567015528678894, 'learning_rate': 0.0007800000000000001, 'epoch': 0.22}\n",
      "{'loss': 4.1163, 'grad_norm': 0.35326823592185974, 'learning_rate': 0.0007775, 'epoch': 0.22}\n",
      "{'loss': 4.1064, 'grad_norm': 0.4156421422958374, 'learning_rate': 0.0007750000000000001, 'epoch': 0.23}\n",
      "{'loss': 4.0808, 'grad_norm': 0.6364544630050659, 'learning_rate': 0.0007725, 'epoch': 0.23}\n",
      "{'loss': 4.0961, 'grad_norm': 0.4769771099090576, 'learning_rate': 0.0007700000000000001, 'epoch': 0.23}\n",
      "{'loss': 4.0436, 'grad_norm': 0.3822889029979706, 'learning_rate': 0.0007675, 'epoch': 0.23}\n",
      "{'loss': 4.0695, 'grad_norm': 0.2915343642234802, 'learning_rate': 0.0007650000000000001, 'epoch': 0.23}\n",
      "{'loss': 4.0614, 'grad_norm': 0.3840959668159485, 'learning_rate': 0.0007624999999999999, 'epoch': 0.24}\n",
      "{'loss': 4.0693, 'grad_norm': 0.39880847930908203, 'learning_rate': 0.00076, 'epoch': 0.24}\n",
      "{'loss': 4.0597, 'grad_norm': 0.4108204245567322, 'learning_rate': 0.0007574999999999999, 'epoch': 0.24}\n",
      "{'loss': 4.0746, 'grad_norm': 0.31859850883483887, 'learning_rate': 0.000755, 'epoch': 0.24}\n",
      "{'loss': 4.0602, 'grad_norm': 0.4068492352962494, 'learning_rate': 0.0007524999999999999, 'epoch': 0.25}\n",
      "{'loss': 4.0604, 'grad_norm': 0.3823172450065613, 'learning_rate': 0.00075, 'epoch': 0.25}\n",
      "{'loss': 4.0738, 'grad_norm': 0.37440985441207886, 'learning_rate': 0.0007475000000000001, 'epoch': 0.25}\n",
      "{'loss': 4.0689, 'grad_norm': 0.3725821375846863, 'learning_rate': 0.000745, 'epoch': 0.26}\n",
      "{'loss': 4.0582, 'grad_norm': 0.4119521379470825, 'learning_rate': 0.0007425000000000001, 'epoch': 0.26}\n",
      "{'loss': 4.0519, 'grad_norm': 0.4765927493572235, 'learning_rate': 0.00074, 'epoch': 0.26}\n",
      "{'loss': 4.056, 'grad_norm': 0.3648660182952881, 'learning_rate': 0.0007375000000000001, 'epoch': 0.26}\n",
      "{'loss': 4.0678, 'grad_norm': 0.39268505573272705, 'learning_rate': 0.000735, 'epoch': 0.27}\n",
      "{'loss': 4.0393, 'grad_norm': 0.42341193556785583, 'learning_rate': 0.0007325000000000001, 'epoch': 0.27}\n",
      "{'loss': 4.0635, 'grad_norm': 0.4065741300582886, 'learning_rate': 0.00073, 'epoch': 0.27}\n",
      "{'loss': 4.0663, 'grad_norm': 0.4881909489631653, 'learning_rate': 0.0007275000000000001, 'epoch': 0.27}\n",
      "{'loss': 4.0416, 'grad_norm': 0.38993293046951294, 'learning_rate': 0.000725, 'epoch': 0.28}\n",
      "{'loss': 4.0168, 'grad_norm': 0.5191437602043152, 'learning_rate': 0.0007225, 'epoch': 0.28}\n",
      " 28%|█████████▍                        | 27822/100000 [50:31<2:07:42,  9.42it/s]^C\n"
     ]
    }
   ],
   "source": [
    "!python /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py --config /home/arda/Documents/chronos-forecasting/notebooks/distillation_copy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-07 13:18:16,911 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Using SEED: 2714500561\n",
      "2025-01-07 13:18:16,913 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Logging dir: output/run-28\n",
      "2025-01-07 13:18:16,913 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Loading and filtering 2 datasets for training: ['/home/arda/Documents/chronos-forecasting/data/tsmixup-data-10percent.arrow', '/home/arda/Documents/chronos-forecasting/data/kernelsynth-data-10percent.arrow']\n",
      "2025-01-07 13:18:16,913 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Mixing probabilities: [0.9, 0.1]\n",
      "2025-01-07 13:18:16,914 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Initializing model\n",
      "2025-01-07 13:18:16,914 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Using random initialization\n",
      "2025-01-07 13:18:48,359 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Loading teacher model\n",
      "2025-01-07 13:18:49,174 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Model dtype: torch.float32\n",
      "2025-01-07 13:18:49,204 - /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py - INFO - Training\n",
      "  0%|                                                | 0/100000 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "{'task_loss': 0.033251708984375, 'encoder_loss': 4.09787548828125, 'decoder_loss': 0.5284404907226562, 'distill_loss': 0.033266368865966796, 'epoch': 0}\n",
      "{'loss': 11.1059, 'grad_norm': 0.35904034972190857, 'learning_rate': 0.0009975000000000001, 'epoch': 0.0}\n",
      "{'task_loss': 8.255365726470947, 'encoder_loss': 574.2499624023437, 'decoder_loss': 55.34625653076172, 'distill_loss': 8.28887794494629, 'epoch': 0.0}\n",
      "{'loss': 10.1561, 'grad_norm': 0.25244343280792236, 'learning_rate': 0.000995, 'epoch': 0.01}\n",
      "{'task_loss': 7.860796754837036, 'encoder_loss': 472.98246472167966, 'decoder_loss': 39.78394613647461, 'distill_loss': 8.091313003540039, 'epoch': 0.01}\n",
      "{'loss': 9.0947, 'grad_norm': 0.32319727540016174, 'learning_rate': 0.0009925000000000001, 'epoch': 0.01}\n",
      "{'task_loss': 6.633302900314331, 'encoder_loss': 448.3542513427734, 'decoder_loss': 38.693209503173826, 'distill_loss': 7.372267250061035, 'epoch': 0.01}\n",
      "{'loss': 8.2314, 'grad_norm': 0.25359204411506653, 'learning_rate': 0.00099, 'epoch': 0.01}\n",
      "{'task_loss': 5.7294514312744145, 'encoder_loss': 432.28986645507814, 'decoder_loss': 37.453585250854495, 'distill_loss': 6.692917415618896, 'epoch': 0.01}\n",
      "{'loss': 7.7075, 'grad_norm': 0.24957814812660217, 'learning_rate': 0.0009875, 'epoch': 0.01}\n",
      "{'task_loss': 5.254250465393066, 'encoder_loss': 416.1842800292969, 'decoder_loss': 35.14015743255615, 'distill_loss': 6.317397546768189, 'epoch': 0.01}\n",
      "{'loss': 7.3621, 'grad_norm': 0.30456477403640747, 'learning_rate': 0.000985, 'epoch': 0.01}\n",
      "{'task_loss': 4.977624090194702, 'encoder_loss': 403.6522196044922, 'decoder_loss': 32.91212620544434, 'distill_loss': 6.079645044326782, 'epoch': 0.01}\n",
      "{'loss': 7.1475, 'grad_norm': 0.22488880157470703, 'learning_rate': 0.0009825, 'epoch': 0.02}\n",
      "{'task_loss': 4.809598459243775, 'encoder_loss': 392.9509645996094, 'decoder_loss': 32.44075218963623, 'distill_loss': 5.898665626525879, 'epoch': 0.02}\n",
      "{'loss': 6.9721, 'grad_norm': 0.30681219696998596, 'learning_rate': 0.00098, 'epoch': 0.02}\n",
      "{'task_loss': 4.702575613975525, 'encoder_loss': 386.94840148925783, 'decoder_loss': 30.903994697570802, 'distill_loss': 5.7608561553955075, 'epoch': 0.02}\n",
      "{'loss': 6.8843, 'grad_norm': 0.24918608367443085, 'learning_rate': 0.0009775, 'epoch': 0.02}\n",
      "{'task_loss': 4.653465920448303, 'encoder_loss': 378.26191650390626, 'decoder_loss': 30.47060944366455, 'distill_loss': 5.698477573394776, 'epoch': 0.02}\n",
      "{'loss': 6.7668, 'grad_norm': 0.28521832823753357, 'learning_rate': 0.000975, 'epoch': 0.03}\n",
      "{'task_loss': 4.580361293792724, 'encoder_loss': 367.69291760253907, 'decoder_loss': 29.61618531036377, 'distill_loss': 5.633363433837891, 'epoch': 0.03}\n",
      "{'loss': 6.6744, 'grad_norm': 0.32608118653297424, 'learning_rate': 0.0009725000000000001, 'epoch': 0.03}\n",
      "{'task_loss': 4.547399369239807, 'encoder_loss': 360.4792547607422, 'decoder_loss': 28.272677352905273, 'distill_loss': 5.5843207530975345, 'epoch': 0.03}\n",
      "{'loss': 6.5958, 'grad_norm': 0.299041748046875, 'learning_rate': 0.0009699999999999999, 'epoch': 0.03}\n",
      "{'task_loss': 4.502884187698364, 'encoder_loss': 352.27961328125, 'decoder_loss': 28.052701217651368, 'distill_loss': 5.525921432495117, 'epoch': 0.03}\n",
      "{'loss': 6.5393, 'grad_norm': 0.31122466921806335, 'learning_rate': 0.0009675, 'epoch': 0.03}\n",
      "{'task_loss': 4.448508096694947, 'encoder_loss': 345.49523352050784, 'decoder_loss': 28.150173355102538, 'distill_loss': 5.494048564910889, 'epoch': 0.03}\n",
      "{'loss': 6.4406, 'grad_norm': 0.40497538447380066, 'learning_rate': 0.000965, 'epoch': 0.04}\n",
      "{'task_loss': 4.396747170448303, 'encoder_loss': 340.55561352539064, 'decoder_loss': 26.796038833618162, 'distill_loss': 5.439241121292114, 'epoch': 0.04}\n",
      "{'loss': 6.3761, 'grad_norm': 0.43608343601226807, 'learning_rate': 0.0009625, 'epoch': 0.04}\n",
      "{'task_loss': 4.3522486324310306, 'encoder_loss': 333.6889149169922, 'decoder_loss': 26.8241826171875, 'distill_loss': 5.390981491088867, 'epoch': 0.04}\n",
      "{'loss': 6.3432, 'grad_norm': 0.35256168246269226, 'learning_rate': 0.00096, 'epoch': 0.04}\n",
      "{'task_loss': 4.328794834136963, 'encoder_loss': 328.0493211669922, 'decoder_loss': 26.58736813735962, 'distill_loss': 5.385485908508301, 'epoch': 0.04}\n",
      "{'loss': 6.3007, 'grad_norm': 0.4663933217525482, 'learning_rate': 0.0009575, 'epoch': 0.04}\n",
      "{'task_loss': 4.304791882514953, 'encoder_loss': 325.8935720214844, 'decoder_loss': 26.010740158081056, 'distill_loss': 5.370664974212646, 'epoch': 0.04}\n",
      "{'loss': 6.25, 'grad_norm': 0.41200128197669983, 'learning_rate': 0.000955, 'epoch': 0.04}\n",
      "{'task_loss': 4.289860523223877, 'encoder_loss': 322.12708630371094, 'decoder_loss': 25.191575130462645, 'distill_loss': 5.336506984710693, 'epoch': 0.04}\n",
      "{'loss': 6.2216, 'grad_norm': 0.39533859491348267, 'learning_rate': 0.0009525, 'epoch': 0.05}\n",
      "{'task_loss': 4.259257433891296, 'encoder_loss': 318.93618237304685, 'decoder_loss': 25.27669903564453, 'distill_loss': 5.326501960754395, 'epoch': 0.05}\n",
      "  5%|█▋                                 | 4889/100000 [21:40<6:55:41,  3.81it/s]"
     ]
    }
   ],
   "source": [
    "!python /home/arda/Documents/chronos-forecasting/scripts/training/distillation_finetuning.py --config /home/arda/Documents/chronos-forecasting/notebooks/distillation_copy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
