{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from gluonts.dataset.arrow import ArrowWriter\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "def convert_to_arrow_chunked(\n",
    "    path: Union[str, Path],\n",
    "    dataset,\n",
    "    chunk_size: int = 10000,\n",
    "    compression: str = \"lz4\",\n",
    "):\n",
    "\n",
    "    \n",
    "    first_chunk = True\n",
    "    \n",
    "    with pa.OSFile(path, 'wb') as f:\n",
    "        for i in range(0, len(dataset), chunk_size):\n",
    "            chunk = dataset[i:i + chunk_size]\n",
    "            chunk_data = []\n",
    "            \n",
    "            # Get the values as lists\n",
    "            ids = chunk['id']\n",
    "            targets = chunk['target']\n",
    "            timestamps = chunk['timestamp']\n",
    "            # Zip the values together and process each row\n",
    "            for id_val, target_val, timestamp_val in zip(ids, targets, timestamps):\n",
    "                chunk_data.append({\n",
    "                    \"start\": timestamp_val[0],\n",
    "                    \"target\": target_val\n",
    "                })\n",
    "            \n",
    "            # Convert to Arrow table\n",
    "            df_chunk = pd.DataFrame(chunk_data)\n",
    "            table_chunk = pa.Table.from_pandas(df_chunk)\n",
    "            \n",
    "            # Initialize writer with schema from first chunk\n",
    "            if first_chunk:\n",
    "                schema = table_chunk.schema\n",
    "                writer = pa.ipc.new_file(f, schema)\n",
    "                first_chunk = False\n",
    "            \n",
    "            # Write chunk\n",
    "            writer.write(table_chunk)\n",
    "            print(f\"Processed {i + len(chunk)} / {len(dataset)} rows\")\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "# # Load and process dataset\n",
    "ds = datasets.load_dataset(\"autogluon/chronos_datasets\", \"training_corpus_tsmixup_10m\", split=\"train\")\n",
    "ds.set_format(\"numpy\")\n",
    "# Convert to arrow format in chunks\n",
    "convert_to_arrow_chunked(\"./tsmixup-data.arrow\", ds, chunk_size=10000)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
