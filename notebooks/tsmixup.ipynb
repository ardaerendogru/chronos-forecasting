{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from gluonts.dataset.arrow import ArrowWriter\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "def convert_to_arrow_chunked(\n",
    "    path: Union[str, Path],\n",
    "    dataset,\n",
    "    chunk_size: int = 10000,\n",
    "    compression: str = \"lz4\",\n",
    "):\n",
    "\n",
    "    \n",
    "    first_chunk = True\n",
    "    \n",
    "    with pa.OSFile(path, 'wb') as f:\n",
    "        for i in range(0, len(dataset), chunk_size):\n",
    "            chunk = dataset[i:i + chunk_size]\n",
    "            chunk_data = []\n",
    "            \n",
    "            # Get the values as lists\n",
    "            ids = chunk['id']\n",
    "            targets = chunk['target']\n",
    "            timestamps = chunk['timestamp']\n",
    "            # Zip the values together and process each row\n",
    "            for id_val, target_val, timestamp_val in zip(ids, targets, timestamps):\n",
    "                chunk_data.append({\n",
    "                    \"start\": timestamp_val[0],\n",
    "                    \"target\": target_val\n",
    "                })\n",
    "            \n",
    "            # Convert to Arrow table\n",
    "            df_chunk = pd.DataFrame(chunk_data)\n",
    "            table_chunk = pa.Table.from_pandas(df_chunk)\n",
    "            \n",
    "            # Initialize writer with schema from first chunk\n",
    "            if first_chunk:\n",
    "                schema = table_chunk.schema\n",
    "                writer = pa.ipc.new_file(f, schema)\n",
    "                first_chunk = False\n",
    "            \n",
    "            # Write chunk\n",
    "            writer.write(table_chunk)\n",
    "            print(f\"Processed {i + len(chunk)} / {len(dataset)} rows\")\n",
    "        \n",
    "        writer.close()\n",
    "\n",
    "# # Load and process dataset\n",
    "ds = datasets.load_dataset(\"autogluon/chronos_datasets\", \"training_corpus_tsmixup_10m\", split=\"train\")\n",
    "ds.set_format(\"numpy\")\n",
    "# Convert to arrow format in chunks\n",
    "convert_to_arrow_chunked(\"../data/tsmixup-data.arrow\", ds, chunk_size=10000)\n",
    "# # Load and process dataset\n",
    "ds = datasets.load_dataset(\"autogluon/chronos_datasets\", \"training_corpus_kernel_synth_1m\", split=\"train\")\n",
    "ds.set_format(\"numpy\")\n",
    "convert_to_arrow_chunked(\"../data/kernelsynth-data.arrow\", ds, chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 100 batches out of 1000 total batches\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import random\n",
    "\n",
    "def sample_arrow_file(input_path: str, output_path: str, sample_ratio: float = 0.1):\n",
    "    # Read the input file\n",
    "    reader = pa.ipc.open_file(input_path)\n",
    "    total_rows = reader.num_record_batches\n",
    "    \n",
    "    # Calculate how many batches to keep\n",
    "    num_batches_to_keep = int(total_rows * sample_ratio)\n",
    "    selected_indices = sorted(random.sample(range(total_rows), num_batches_to_keep))\n",
    "    \n",
    "    # Get the schema from the original file\n",
    "    schema = reader.schema\n",
    "    \n",
    "    # Write selected batches to new file\n",
    "    with pa.OSFile(output_path, 'wb') as f:\n",
    "        writer = pa.ipc.new_file(f, schema)\n",
    "        \n",
    "        for idx in selected_indices:\n",
    "            batch = reader.get_batch(idx)\n",
    "            writer.write(batch)\n",
    "        \n",
    "        writer.close()\n",
    "    \n",
    "    print(f\"Sampled {num_batches_to_keep} batches out of {total_rows} total batches\")\n",
    "\n",
    "# Use the function\n",
    "input_file = \"/home/arda/Documents/chronos-forecasting/data/tsmixup-data.arrow\"\n",
    "output_file = \"/home/arda/Documents/chronos-forecasting/data/tsmixup-data-10percent.arrow\"\n",
    "sample_arrow_file(input_file, output_file, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
