teacher_model_path: "amazon/chronos-t5-base"
temperature: 2.0
model_id: "amazon/chronos-t5-tiny"
training_data_paths: ["/home/arda/Documents/chronos-forecasting/data/tsmixup-data.arrow", "/home/arda/Documents/chronos-forecasting/data/kernelsynth-data.arrow"]
probability: [0.9, 0.1]
max_steps: 10_000
per_device_train_batch_size: 32
learning_rate: 1e-4

context_length: 128
prediction_length: 16
min_past: 10
save_steps: 5000
log_steps: 500
optim: adamw_torch_fused
num_samples: 20
shuffle_buffer_length: 100_000
gradient_accumulation_steps: 1
model_type: seq2seq
random_init: true
tie_embeddings: true
output_dir: ./output/
tf32: true
torch_compile: true
tokenizer_class: "MeanScaleUniformBins"
tokenizer_kwargs:
  low_limit: -15.0
  high_limit: 15.0
n_tokens: 4096
lr_scheduler_type: linear
warmup_ratio: 0.0
dataloader_num_workers: 1
max_missing_prop: 0.9
use_eos_token: true

alpha: 0.5
student_hidden_size: 256  #{tiny:256, mini:384, small:512, base:768, large:1024, }
teacher_hidden_size: 768  
distill_temperature: 2
kl_loss_weight: 0.5
task_loss_weight: 0.5
encoder_loss_weight: 0.01
decoder_loss_weight: 0.1